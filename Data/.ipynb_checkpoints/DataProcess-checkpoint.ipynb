{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to install these packages in terminal in your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "import pickle\n",
    "import csv\n",
    "#print(sys.version_info[0])\n",
    "\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genism\n",
    "Here we add Gensim libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import gensim \n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import coherencemodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim # don't skip this \n",
    "# import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable logging for gensim - optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Stopwords\n",
    "We have already downloaded the stopwords. Let’s import them and make it available in stop_words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/CAMPUS/nabieir/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay, the file exists!\n"
     ]
    }
   ],
   "source": [
    "# from pathlib import Path\n",
    "import os,sys\n",
    "from pathlib import Path\n",
    "curr_dir = os.getcwd() + '/'\n",
    "\n",
    "filename = curr_dir + \"../Data/Outputs/YelpClinicalReview/clinical_reviews_texted.txt\" #Path to .txt data file one document per sentence\n",
    "if not os.path.isfile(filename):\n",
    "    print(\"Oops, file doesn't exist!\")\n",
    "else:\n",
    "    print(\"Yay, the file exists!\")\n",
    "#Read input .txt data and load to an array\n",
    "with open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "    data = f.read().splitlines()\n",
    "\n",
    "    \n",
    "def savetofile (fpath,data):\n",
    "    with open(fpath,\"wt\") as f:\n",
    "        for sent in data:\n",
    "            f.write(sent + '\\n')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting non-English sentences \n",
    "We can use polyglot or langdetect package. polyglot is faster and can be used for name-entity detection as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## detecting non-english sentences\n",
    "from polyglot.detect import Detector\n",
    "non_english = {}\n",
    "\n",
    "for i,sent in enumerate(data):\n",
    "    detector = Detector(sent,quiet=True)\n",
    "    if detector.language.code != 'en':\n",
    "        non_english[i] = sent\n",
    "#print(len(non_english),len(data))\n",
    "\n",
    "## save only english text as a new source text data \n",
    "\n",
    "# for index in sorted(non_english, reverse=True):     #Delete detected non_english sentences\n",
    "#     del data[index]\n",
    "    \n",
    "# with open(curr_dir + \"data/clinical_reviews_only_english.txt\",\"wt\") as f:\n",
    "#     for sent in data:\n",
    "#         f.write(sent + '\\n')\n",
    "\n",
    "##### This is another package langdetect but it is slower !!!\n",
    "\n",
    "# from langdetect import detect\n",
    "\n",
    "# non_english = []\n",
    "\n",
    "# for i,sent in enumerate(data):\n",
    "#     detector = detect(sent)\n",
    "#     if detector != 'en':\n",
    "#         non_english.append(sent)\n",
    "#         print(i,sent)\n",
    "        \n",
    "# print(len(non_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find abbriviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A = \"salam che LSB khoobi ?\"\n",
    "# test = re.findall('([A-Z]+)', A)\n",
    "# print(test)\n",
    "# import re\n",
    "# for i,sent in enumerate(data):\n",
    "#     abbr = re.findall(r'\\b[A-Z][A-Z]+\\b',sent)\n",
    "#     if len(abbr) > 0:\n",
    "#         print (i,abbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"lenght of data was :\" + str(len(data)))\n",
    "# # Deleting very small sentences (Document)\n",
    "# #data1 = [1, 3, 6, 35, 3 ,47,9,15]\n",
    "# # print(data1)\n",
    "# i = 0\n",
    "# for n in data[:]:\n",
    "#     if len(n) < 200:\n",
    "#         print(n)\n",
    "#         data.remove(n)\n",
    "        \n",
    "# savetofile(curr_dir + \"data/clinical_reviews_only_english_200char.txt\",data)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import Patient review data from Yelp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CountChar(List):      # funtion to count all the characters in a list\n",
    "#     sum = 0\n",
    "#     for line in List:\n",
    "#         sum += len(line)\n",
    "#     return sum\n",
    "# # OriginalLenght = sum(len(line) for line in List) # this is more pythonic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Tokenize words and Clean-up text\n",
    "Let’s tokenize each sentence into a list of words, removing punctuations and unnecessary characters altogether.\n",
    "\n",
    "Gensim’s simple_preprocess() is great for this. Additionally I have set deacc=True to remove the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for i,sentence in enumerate(sentences):\n",
    "#         if i % 100000 == 0:    # this is just for debugging when the database is very big\n",
    "#             print (i/100000)\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "# TODO count the number of sentences         \n",
    "data_words = list(sent_to_words(data))\n",
    "#data_words = list(filter(None, data_words)) #_ remove empty cells in the list\n",
    "#data = list(filter(None, data)) #_ remove empty cells in the list\n",
    "#print(len(data))\n",
    "# TODO count the number of sentences maybe compare them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inthenameofgodr', 'the', 'lord', 'name', 'of', 'god']\n"
     ]
    }
   ],
   "source": [
    "print(gensim.utils.simple_preprocess('InthenameofGodR the, 1lord name of God!', deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(sum(len(sen) for sen in data))\n",
    "# print(CountChar(data)) \n",
    "# # print(sum(len(words) for words in data_words))\n",
    "# print(CountChar(data_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Creating Bigram and Trigram Models\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring.\n",
    "\n",
    "Some examples in our patient review data are: ‘weight_loss’, ‘weigth_training’, 'most_likely' etc.\n",
    "\n",
    "Gensim’s Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold. The higher the values of these param, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/CAMPUS/nabieir/anaconda3/envs/python3.6/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=80)# higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "# Print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(bigram_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(bigram_mod[data_words[59]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. Remove Stopwords, Make Bigrams and Lemmatize\n",
    "The bigrams model is ready. Let’s define the functions to remove the stopwords, make bigrams and lemmatization and call them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s call the functions in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['physical_therapy']]"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_bigrams([[\"physical\",\"therapy\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Create the Dictionary and Corpus needed for Topic Modeling\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "#_ Joiining words together to build sentences this will be used later \n",
    "data = [' '.join(lemma) for lemma in data_lemmatized] \n",
    "\n",
    "# View\n",
    "# print(corpus[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd         #Find the term-frequency in dictionary        \n",
    "# # vocab = list(id2word.values()) #list of terms in the dictionary\n",
    "# vocab_tf = [dict(i) for i in corpus]\n",
    "# vocab_tf = list(pd.DataFrame(vocab_tf).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "go\n"
     ]
    }
   ],
   "source": [
    "# # rare_tokens = [index for index in range(len(vocab_tf)) if vocab_tf[index]<4] #index of the word that appear less than 3 times in the corpus\n",
    "# print(type(vocab_tf))\n",
    "# print(id2word[max(range(len(vocab_tf)), key=vocab_tf.__getitem__)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([id2word[index] for index in one_tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can see a human-readable form of the corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "# [[(id2word[id], freq) for id, freq in cp] for cp in corpus[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15580\n",
      "15580\n",
      "54.70783055198973\n"
     ]
    }
   ],
   "source": [
    "# print(len(data_lemmatized))\n",
    "# print(len(corpus))\n",
    "# AVG = sum (len(Sen) for Sen in data_lemmatized)/len(data_lemmatized)\n",
    "# print(AVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LDA Mallet Model\n",
    "Gensim provides a wrapper to implement Mallet’s LDA from within Gensim itself. You only need to download the zipfile, unzip it and provide the path to mallet in the unzipped directory to gensim.models.wrappers.LdaMallet. See how I have done this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "# mallet_path = '/home/CAMPUS/nabieir/anaconda3/mallet-2.0.8/bin/mallet' # update this path\n",
    "# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the optimal number of topics for LDA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = '/home/CAMPUS/nabieir/anaconda3/mallet-2.0.8/bin/mallet' # update this path\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \n",
    "    NOTE, The Coherence calcualation has been commented out, and calculated separately later\n",
    "    \"\"\"\n",
    "    #coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        start_time = time.time()\n",
    "        print(\"Training LDA with %d topics starts ...\" % num_topics )\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        #coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        #coherence_ldamallet = coherence_model.get_coherence()\n",
    "        #coherence_values.append(coherence_ldamallet)\n",
    "        print(\"LDA training time = %s min\" % ((time.time() - start_time)/60))\n",
    "        #print('Coherence Score: ', coherence_ldamallet)\n",
    "    return model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=10, limit=220, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save our models \n",
    "import os\n",
    "directory = curr_dir + 'MalletModels/'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "for i,model in enumerate(model_list):\n",
    "    filename =  directory + str(i) + \".mallet\" \n",
    "    model.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load our models \n",
    "# model_list = []\n",
    "# for i in range(20):\n",
    "#     filename =  current + \"MalletModels/\" + str(i) + \".mallet\"\n",
    "#     model_list.append(gensim.models.wrappers.LdaMallet.load(filename,mmap='r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_list)\n",
    "# len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_values = []\n",
    "for Mmodel in model_list:\n",
    "    coherence_model = gensim.models.CoherenceModel(model=Mmodel, texts=data_lemmatized, dictionary=id2word, coherence='c_v', topn = 10)\n",
    "    coherence_ldamallet = coherence_model.get_coherence()\n",
    "    coherence_values.append(coherence_ldamallet)\n",
    "    print('Coherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# This is to save the Coherence_values to a text file\n",
    "print(len(coherence_values))\n",
    "with open(curr_dir + \"MalletModels/CV_Coherence_topn10.txt\",\"wt\") as f:\n",
    "    for C in coherence_values:\n",
    "        f.write(str(C) +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Coherence_values form a text file to a list\n",
    "# coherence_values = []\n",
    "# with open(\"/home/CAMPUS/nabieir/HDD/LDA/Gensim-LDA/MalletModels/Coherences.txt\", \"r\") as f:\n",
    "#     for line in f:coherence_values.append(float(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=220; start=10; step=10;\n",
    "x = range(start, limit, step)\n",
    "num_topic = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 20))\n",
    "# ax.grid(True)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score (C_V)\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(num_topic, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('life', 0.08423256563053631), ('health', 0.04469318730874537), ('year', 0.037928813013367695), ('change', 0.03511032372362699), ('condition', 0.026896440650668384), ('treat', 0.020293122886133032), ('healthy', 0.02021259462071187), ('live', 0.017957803188919312), ('save', 0.017555161861813495), ('trust', 0.013689805121597681)]\n"
     ]
    }
   ],
   "source": [
    "#type(optimal_model)\n",
    "#optimal_model.show_topic(20,topn=10)\n",
    "print(topic_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the highest coherence and print the topics\n",
    "# optimal_model = model_list[max(range(len(coherence_values)), key=coherence_values.__getitem__)]\n",
    "\n",
    "# roo = dict(sorted(optimal_model.show_topics(num_topics=50, num_words=10,formatted=False),key=lambda x: (x[0]))))\n",
    "topics_list = []\n",
    "for x in sorted(optimal_model.show_topics(num_topics=50,num_words=10,formatted=False),key=lambda x: (x[0])):\n",
    "     topic_list.append(x[1])\n",
    "    \n",
    "word_topic_matrix = np.zeros((50,len(id2word)))\n",
    "\n",
    "for y in range(50):\n",
    "    row = optimal_model.show_topic(y,topn=len(id2word))\n",
    "    for q,(x,z) in enumerate(row):\n",
    "        word_topic_matrix[y][q] = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.08423256563053631),\n",
       " ('health', 0.04469318730874537),\n",
       " ('year', 0.037928813013367695),\n",
       " ('change', 0.03511032372362699),\n",
       " ('condition', 0.026896440650668384),\n",
       " ('treat', 0.020293122886133032),\n",
       " ('healthy', 0.02021259462071187),\n",
       " ('live', 0.017957803188919312),\n",
       " ('save', 0.017555161861813495),\n",
       " ('trust', 0.013689805121597681)]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the dominant topic in each sentence\n",
    "One of the practical application of topic modeling is to determine what topic a given document is about.\n",
    "To find that, we find the topic number that has the highest percentage contribution in that document.\n",
    "\n",
    "The format_topics_sentences() function below nicely aggregates this information in a presentable table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #optimal_model[corpus[1]]\n",
    "# print(len(data))\n",
    "# print(len(corpus))\n",
    "# print(len(df_dominant_topic))\n",
    "# df_dominant_topic.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dominant_topic.to_pickle(\"df_dominant_topic.pkl\")\n",
    "# df_topic_sents_keywords.to_pickle(\"df_topic_sents_keywords.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A  = df_dominant_topic[['Dominant_Topic','Keywords']]  # prevelant to save topics first 10 words to CSV file\n",
    "B = A.drop_duplicates(subset='Dominant_Topic', keep=\"first\") \n",
    "B.sort_values('Dominant_Topic').to_csv(\"Keywords11.csv\",doublequote = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_topic_matrix = np.zeros((50,len(corpus)))\n",
    "# for i, row in enumerate(optimal_model[corpus]):\n",
    "#     row = sorted(row, key=lambda x: (x[0]))\n",
    "#     for topic,prop in row:\n",
    "#         doc_topic_matrix[topic][i] = prop\n",
    "        \n",
    "# np.save(curr_dir + \"doc_topic_matrix.npy\",doc_topic_matrix, allow_pickle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [z for z in range(5) if z != 3] \n",
    "3 in a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_discriminative_score(word, topic_id):\n",
    "    try:\n",
    "        word_id = id2word.token2id[word]\n",
    "        p_w_given_t = word_topic_matrix[topic_id][word_id]\n",
    "    except KeyError:\n",
    "        print(\"\\\"\" + word + \"\\\"\" + ' is not in list of vocabulary')\n",
    "        return 0;\n",
    "    \n",
    "    t_prime =[word_topic_matrix[z][word_id] for z in range(50) if z != topic_id] \n",
    "    p_w_given_t_prime = max(t_prime)\n",
    "    \n",
    "    return p_w_given_t/p_w_given_t_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_discriminative_score(sentence, topic_id):\n",
    "    sentence_score = 0\n",
    "    for word in sentence:\n",
    "        sentence_score += w_discriminative_score(word,topic_id)\n",
    "    return sentence_score/len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adult year become try true doctor snob wait extend period time hate call schedule appointment sick tell see week loathe feeling doctor care gibson staff may psychic surpass expectation start finish sit wait room minute call back nurse pam legit sweet friendly make sure comfortable gibson lovely spend minute room actively listen get know assure set office accommodate day appointment urgent case love also lab work house happy relieve finally find pcp care patient five_star doctor five_star office five_star experience'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Very Important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_list[0])\n",
    "print('\\n'.join([data[i] for i in top10_doc[0][:4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0726</td>\n",
       "      <td>call, receive, send, return, request, referral...</td>\n",
       "      <td>never know awesome service staff shit hit fan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0615</td>\n",
       "      <td>wait, hour, room, minute, long, time, sit, hal...</td>\n",
       "      <td>adult year become try true doctor snob wait ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>surgery, ray, foot, procedure, week, hand, bre...</td>\n",
       "      <td>johnsen treat sports_relat knee injury profess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0611</td>\n",
       "      <td>doctor, find, office, patient, nurse, talk, fi...</td>\n",
       "      <td>absolute_worst doctor office vegas wait hour e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>surgery, ray, foot, procedure, week, hand, bre...</td>\n",
       "      <td>go liu seem great doctor office staff rude try...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>feel, make, question, answer, comfortable, rus...</td>\n",
       "      <td>owner joe thorough care client make feel comfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>great, staff, friendly, experience, job, highl...</td>\n",
       "      <td>jone great personable really spend time get kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>chiropractor, neck, chiropractic, back, adjust...</td>\n",
       "      <td>second baby attempt hit gym get back shape bod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0791</td>\n",
       "      <td>good, recommend, find, friend, feel, wife, hap...</td>\n",
       "      <td>lee recommend friend bother shoulder pain weig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>massage, therapist, feel, session, body, press...</td>\n",
       "      <td>impressed search good massage arrive phoenix a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>time, long, spend, wait, waste, short, busy, d...</td>\n",
       "      <td>bad wait time always long urgent care facility...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>prescription, doctor, medication, med, give, p...</td>\n",
       "      <td>go pain center arizona matthew doust treat mot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>place, nice, good, great, clean, awesome, supe...</td>\n",
       "      <td>office awesome paint everywhere fun cartoon ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>care, health, urgent, provider, primary, patie...</td>\n",
       "      <td>turntable first open fantastic however go way ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>bad, place, people, rude, horrible, front_desk...</td>\n",
       "      <td>place absolutely horrible receptionist try avo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>place, nice, good, great, clean, awesome, supe...</td>\n",
       "      <td>regret go dr skabo refer friend never let go w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.1423</td>\n",
       "      <td>physical_therapy, therapist, therapy, exercise...</td>\n",
       "      <td>year ago need physical_therapy know henceforth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>professional, extremely, knowledgeable, recomm...</td>\n",
       "      <td>great friendly people really care recovery see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>class, studio, room, clean, water, instructor,...</td>\n",
       "      <td>review nurse_practitioner courtney drive minut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>result, test, blood, lab, work, order, follow,...</td>\n",
       "      <td>tice excellent doctor treat metabolic hormonal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0             0             8.0              0.0726   \n",
       "1             1            18.0              0.0615   \n",
       "2             2            31.0              0.0660   \n",
       "3             3            12.0              0.0611   \n",
       "4             4            31.0              0.0481   \n",
       "5             5            28.0              0.0462   \n",
       "6             6            45.0              0.0547   \n",
       "7             7            39.0              0.1035   \n",
       "8             8            13.0              0.0791   \n",
       "9             9            41.0              0.0869   \n",
       "10           10            30.0              0.0528   \n",
       "11           11            49.0              0.1020   \n",
       "12           12            16.0              0.0757   \n",
       "13           13             5.0              0.1320   \n",
       "14           14            34.0              0.0670   \n",
       "15           15            16.0              0.0710   \n",
       "16           16            43.0              0.1423   \n",
       "17           17            32.0              0.0539   \n",
       "18           18            47.0              0.0604   \n",
       "19           19            36.0              0.2139   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   call, receive, send, return, request, referral...   \n",
       "1   wait, hour, room, minute, long, time, sit, hal...   \n",
       "2   surgery, ray, foot, procedure, week, hand, bre...   \n",
       "3   doctor, find, office, patient, nurse, talk, fi...   \n",
       "4   surgery, ray, foot, procedure, week, hand, bre...   \n",
       "5   feel, make, question, answer, comfortable, rus...   \n",
       "6   great, staff, friendly, experience, job, highl...   \n",
       "7   chiropractor, neck, chiropractic, back, adjust...   \n",
       "8   good, recommend, find, friend, feel, wife, hap...   \n",
       "9   massage, therapist, feel, session, body, press...   \n",
       "10  time, long, spend, wait, waste, short, busy, d...   \n",
       "11  prescription, doctor, medication, med, give, p...   \n",
       "12  place, nice, good, great, clean, awesome, supe...   \n",
       "13  care, health, urgent, provider, primary, patie...   \n",
       "14  bad, place, people, rude, horrible, front_desk...   \n",
       "15  place, nice, good, great, clean, awesome, supe...   \n",
       "16  physical_therapy, therapist, therapy, exercise...   \n",
       "17  professional, extremely, knowledgeable, recomm...   \n",
       "18  class, studio, room, clean, water, instructor,...   \n",
       "19  result, test, blood, lab, work, order, follow,...   \n",
       "\n",
       "                                                 Text  \n",
       "0   never know awesome service staff shit hit fan ...  \n",
       "1   adult year become try true doctor snob wait ex...  \n",
       "2   johnsen treat sports_relat knee injury profess...  \n",
       "3   absolute_worst doctor office vegas wait hour e...  \n",
       "4   go liu seem great doctor office staff rude try...  \n",
       "5   owner joe thorough care client make feel comfo...  \n",
       "6   jone great personable really spend time get kn...  \n",
       "7   second baby attempt hit gym get back shape bod...  \n",
       "8   lee recommend friend bother shoulder pain weig...  \n",
       "9   impressed search good massage arrive phoenix a...  \n",
       "10  bad wait time always long urgent care facility...  \n",
       "11  go pain center arizona matthew doust treat mot...  \n",
       "12  office awesome paint everywhere fun cartoon ke...  \n",
       "13  turntable first open fantastic however go way ...  \n",
       "14  place absolutely horrible receptionist try avo...  \n",
       "15  regret go dr skabo refer friend never let go w...  \n",
       "16  year ago need physical_therapy know henceforth...  \n",
       "17  great friendly people really care recovery see...  \n",
       "18  review nurse_practitioner courtney drive minut...  \n",
       "19  tice excellent doctor treat metabolic hormonal...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic_Roo\n",
    "for i in [5,7,9]:\n",
    "    if i != 5:\n",
    "        df_topic_Roo = df_topic_Roo.append(format_topics_sentences(ldamodel=model_list[i], corpus=corpus, texts=data))\n",
    "\n",
    "# Format\n",
    "    df_dominant_topic_Roo[i] = df_topic_sents_keywords.reset_index()\n",
    "    df_dominant_topic_Roo[i].columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dominant_topic.iloc[7][4])\n",
    "print(sorted(optimal_model[corpus][4], key=lambda x: x[1], reverse= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_dominant_topic.iloc[10,3])\n",
    "print(df_dominant_topic.iloc[5,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model.show_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most representative document for each topic\n",
    "Sometimes just the topic keywords may not be enough to make sense of what a topic is about. So, to help with understanding the topic, you can find the documents a given topic has contributed to the most and infer the topic by reading that document. Whew!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent_topics_sorteddf_mallet.iloc[20][2])\n",
    "print(sent_topics_sorteddf_mallet.iloc[20][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic distribution across documents\n",
    "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [] \n",
    "a = a.append('333')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
